#!/usr/bin/env python3
"""
BroadenAgenticæ¡†æ¶å¿«é€Ÿæµ‹è¯•
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.core.llm import LLMFactory, LLMFactoryConfig


async def quick_test():
    """å¿«é€Ÿæµ‹è¯•"""
    print("ğŸš€ BroadenAgenticæ¡†æ¶å¿«é€Ÿæµ‹è¯•")
    print("=" * 40)
    
    api_key = "sk-9b6f66d94dd249dfb7a7416cc270ce4d"
    
    # æµ‹è¯•è¿é€šæ€§
    print("\nğŸ” è¿é€šæ€§æµ‹è¯•")
    
    # æœ¬åœ°æ¨¡å‹æµ‹è¯•
    print("ğŸ  æµ‹è¯•æœ¬åœ°æ¨¡å‹...")
    try:
        local_config = LLMFactoryConfig(
            preferred_mode="local",
            local_model_name="qwen3:8b"
        )
        local_factory = LLMFactory(local_config)
        local_llm = await local_factory.create_llm()
        
        is_healthy = await local_llm.health_check()
        print(f"   æœ¬åœ°æ¨¡å‹: {'âœ… æ­£å¸¸' if is_healthy else 'âŒ å¼‚å¸¸'}")
        
        if is_healthy:
            result = await local_llm.generate("Hello", max_tokens=10)
            print(f"   æµ‹è¯•å“åº”: {result.text[:50]}...")
            print(f"   ä½¿ç”¨Token: {result.tokens_used}")
            
    except Exception as e:
        print(f"   æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
    
    # äº‘ç«¯æ¨¡å‹æµ‹è¯•
    print("\nâ˜ï¸ æµ‹è¯•äº‘ç«¯æ¨¡å‹...")
    try:
        cloud_config = LLMFactoryConfig(
            preferred_mode="cloud",
            cloud_model_name="qwen-plus",
            api_key=api_key
        )
        cloud_factory = LLMFactory(cloud_config)
        cloud_llm = await cloud_factory.create_llm()
        
        is_healthy = await cloud_llm.health_check()
        print(f"   äº‘ç«¯æ¨¡å‹: {'âœ… æ­£å¸¸' if is_healthy else 'âŒ å¼‚å¸¸'}")
        
        if is_healthy:
            result = await cloud_llm.generate("Hello", max_tokens=10)
            print(f"   æµ‹è¯•å“åº”: {result.text[:50]}...")
            print(f"   ä½¿ç”¨Token: {result.tokens_used}")
            
    except Exception as e:
        print(f"   äº‘ç«¯æ¨¡å‹å¤±è´¥: {e}")
    
    # Tokenç›‘æ§æµ‹è¯•
    print("\nğŸ”¢ Tokenç›‘æ§æµ‹è¯•")
    try:
        cloud_llm.reset_token_usage()
        
        questions = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
            result = await cloud_llm.generate(question)
            print(f"   å“åº”: {result.text[:100]}...")
            print(f"   Token: {result.tokens_used}")
        
        # æ˜¾ç¤ºTokenç»Ÿè®¡
        token_usage = cloud_llm.get_token_usage()
        print(f"\nğŸ“Š Tokenç»Ÿè®¡:")
        print(f"   æ€»Token: {token_usage['total_tokens_used']}")
        print(f"   æ€»è¯·æ±‚: {token_usage['total_requests']}")
        print(f"   å¹³å‡Token/è¯·æ±‚: {token_usage['average_tokens_per_request']:.1f}")
        
    except Exception as e:
        print(f"   Tokenç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(quick_test()) 