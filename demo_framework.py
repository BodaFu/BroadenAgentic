#!/usr/bin/env python3
"""
BroadenAgenticæ¡†æ¶æ¼”ç¤º - ä¼˜åŒ–ç‰ˆæœ¬
å±•ç¤ºæœ¬åœ°Ollamaå’Œäº‘ç«¯Qwen APIçš„ä½¿ç”¨ï¼ŒåŒ…å«Tokenç›‘æ§
"""

import asyncio
import sys
import os
import time
from typing import Dict, Any, Callable

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.core.llm import LLMFactory, LLMFactoryConfig, LLMConfig
from app.core.agent import AgentConfig, AgentBase
from app.core.constraint import InputTypeConstraint, OutputTypeConstraint, OutputCriterion


class DemoFramework:
    """BroadenAgenticæ¡†æ¶æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.api_key = "sk-9b6f66d94dd249dfb7a7416cc270ce4d"
        self.demo_modules = {
            "1": ("è¿é€šæ€§æµ‹è¯•", self.test_connectivity),
            "2": ("æœ¬åœ°æ¨¡å‹æ ·ä¾‹", self.demo_local_model),
            "3": ("äº‘ç«¯æ¨¡å‹æ ·ä¾‹", self.demo_cloud_model),
            "4": ("æ€§èƒ½å¯¹æ¯”æ ·ä¾‹", self.demo_performance_comparison),
            "5": ("Agentæ ·ä¾‹", self.demo_agent),
            "6": ("Tokenç›‘æ§æ ·ä¾‹", self.demo_token_monitoring),
            "7": ("è¿è¡Œæ‰€æœ‰æ ·ä¾‹", self.run_all_demos)
        }
    
    def show_menu(self):
        """æ˜¾ç¤ºèœå•"""
        print("\nğŸ¯ BroadenAgenticæ¡†æ¶æ¼”ç¤º")
        print("=" * 50)
        print("è¯·é€‰æ‹©è¦è¿è¡Œçš„æµ‹è¯•é¡¹ç›®ï¼š")
        print()
        
        for key, (name, _) in self.demo_modules.items():
            print(f"  {key}. {name}")
        
        print("\n  0. é€€å‡º")
        print("=" * 50)
    
    async def run_demo(self, demo_name: str, demo_func: Callable):
        """è¿è¡Œæ¼”ç¤º"""
        print(f"\nğŸš€ å¼€å§‹è¿è¡Œ: {demo_name}")
        print("-" * 40)
        
        try:
            await demo_func()
            print(f"âœ… {demo_name} è¿è¡Œå®Œæˆ")
        except Exception as e:
            print(f"âŒ {demo_name} è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print("-" * 40)
    
    async def test_connectivity(self):
        """æµ‹è¯•æœ¬åœ°å’Œäº‘ç«¯æ¨¡å‹è¿é€šæ€§"""
        print("ğŸ” è¿é€šæ€§æµ‹è¯•")
        
        # æµ‹è¯•æœ¬åœ°æ¨¡å‹
        print("\nğŸ  æµ‹è¯•æœ¬åœ°Ollamaæ¨¡å‹...")
        try:
            local_config = LLMFactoryConfig(
                preferred_mode="local",
                local_model_name="qwen3:8b"
            )
            local_factory = LLMFactory(local_config)
            local_llm = await local_factory.create_llm()
            
            # å¥åº·æ£€æŸ¥
            is_healthy = await local_llm.health_check()
            print(f"   æœ¬åœ°æ¨¡å‹å¥åº·çŠ¶æ€: {'âœ… æ­£å¸¸' if is_healthy else 'âŒ å¼‚å¸¸'}")
            
            if is_healthy:
                # ç®€å•æµ‹è¯•
                result = await local_llm.generate("Hello", max_tokens=10)
                print(f"   æµ‹è¯•å“åº”: {result.text[:50]}...")
                print(f"   ä½¿ç”¨Tokenæ•°: {result.tokens_used}")
            
        except Exception as e:
            print(f"   æœ¬åœ°æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
        
        # æµ‹è¯•äº‘ç«¯æ¨¡å‹
        print("\nâ˜ï¸ æµ‹è¯•äº‘ç«¯Qwen API...")
        try:
            cloud_config = LLMFactoryConfig(
                preferred_mode="cloud",
                cloud_model_name="qwen-plus",
                api_key=self.api_key
            )
            cloud_factory = LLMFactory(cloud_config)
            cloud_llm = await cloud_factory.create_llm()
            
            # å¥åº·æ£€æŸ¥
            is_healthy = await cloud_llm.health_check()
            print(f"   äº‘ç«¯æ¨¡å‹å¥åº·çŠ¶æ€: {'âœ… æ­£å¸¸' if is_healthy else 'âŒ å¼‚å¸¸'}")
            
            if is_healthy:
                # ç®€å•æµ‹è¯•
                result = await cloud_llm.generate("Hello", max_tokens=10)
                print(f"   æµ‹è¯•å“åº”: {result.text[:50]}...")
                print(f"   ä½¿ç”¨Tokenæ•°: {result.tokens_used}")
            
        except Exception as e:
            print(f"   äº‘ç«¯æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
    
    async def demo_local_model(self):
        """æœ¬åœ°æ¨¡å‹æ ·ä¾‹"""
        print("ğŸ  æœ¬åœ°æ¨¡å‹æ ·ä¾‹")
        
        # åˆ›å»ºæœ¬åœ°LLM
        config = LLMFactoryConfig(
            preferred_mode="local",
            local_model_name="qwen3:8b"
        )
        factory = LLMFactory(config)
        llm = await factory.create_llm()
        
        # é‡ç½®Tokenç»Ÿè®¡
        llm.reset_token_usage()
        
        # æµ‹è¯•é—®é¢˜
        questions = [
            "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
            
            start_time = time.time()
            result = await llm.generate(question)
            end_time = time.time()
            
            print(f"   â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"   ğŸ”¢ ä½¿ç”¨Tokenæ•°: {result.tokens_used}")
            print(f"   ğŸ“„ å›ç­”: {result.text[:100]}...")
        
        # æ˜¾ç¤ºTokenç»Ÿè®¡
        token_usage = llm.get_token_usage()
        print(f"\nğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡:")
        print(f"   æ€»Tokenæ•°: {token_usage['total_tokens_used']}")
        print(f"   æ€»è¯·æ±‚æ•°: {token_usage['total_requests']}")
        print(f"   å¹³å‡Token/è¯·æ±‚: {token_usage['average_tokens_per_request']:.1f}")
    
    async def demo_cloud_model(self):
        """äº‘ç«¯æ¨¡å‹æ ·ä¾‹"""
        print("â˜ï¸ äº‘ç«¯æ¨¡å‹æ ·ä¾‹")
        
        # åˆ›å»ºäº‘ç«¯LLM
        config = LLMFactoryConfig(
            preferred_mode="cloud",
            cloud_model_name="qwen-plus",
            api_key=self.api_key
        )
        factory = LLMFactory(config)
        llm = await factory.create_llm()
        
        # é‡ç½®Tokenç»Ÿè®¡
        llm.reset_token_usage()
        
        # æµ‹è¯•é—®é¢˜
        questions = [
            "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
            
            start_time = time.time()
            result = await llm.generate(question)
            end_time = time.time()
            
            print(f"   â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"   ğŸ”¢ ä½¿ç”¨Tokenæ•°: {result.tokens_used}")
            print(f"   ğŸ“„ å›ç­”: {result.text[:100]}...")
        
        # æ˜¾ç¤ºTokenç»Ÿè®¡
        token_usage = llm.get_token_usage()
        print(f"\nğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡:")
        print(f"   æ€»Tokenæ•°: {token_usage['total_tokens_used']}")
        print(f"   æ€»è¯·æ±‚æ•°: {token_usage['total_requests']}")
        print(f"   å¹³å‡Token/è¯·æ±‚: {token_usage['average_tokens_per_request']:.1f}")
    
    async def demo_performance_comparison(self):
        """æ€§èƒ½å¯¹æ¯”æ ·ä¾‹"""
        print("âš¡ æ€§èƒ½å¯¹æ¯”æ ·ä¾‹")
        
        test_question = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œå¹¶ä¸¾ä¾‹è¯´æ˜å…¶åº”ç”¨åœºæ™¯"
        
        # æœ¬åœ°æ¨¡å‹æµ‹è¯•
        print("\nğŸ  æœ¬åœ°æ¨¡å‹æµ‹è¯•...")
        local_config = LLMFactoryConfig(
            preferred_mode="local",
            local_model_name="qwen3:8b"
        )
        local_factory = LLMFactory(local_config)
        local_llm = await local_factory.create_llm()
        local_llm.reset_token_usage()
        
        start_time = time.time()
        local_result = await local_llm.generate(test_question)
        local_time = time.time() - start_time
        
        local_token_usage = local_llm.get_token_usage()
        print(f"   â±ï¸ å“åº”æ—¶é—´: {local_time:.2f}ç§’")
        print(f"   ğŸ”¢ ä½¿ç”¨Tokenæ•°: {local_result.tokens_used}")
        print(f"   ğŸ“Š æ€»Tokenç»Ÿè®¡: {local_token_usage['total_tokens_used']}")
        
        # äº‘ç«¯æ¨¡å‹æµ‹è¯•
        print("\nâ˜ï¸ äº‘ç«¯æ¨¡å‹æµ‹è¯•...")
        cloud_config = LLMFactoryConfig(
            preferred_mode="cloud",
            cloud_model_name="qwen-plus",
            api_key=self.api_key
        )
        cloud_factory = LLMFactory(cloud_config)
        cloud_llm = await cloud_factory.create_llm()
        cloud_llm.reset_token_usage()
        
        start_time = time.time()
        cloud_result = await cloud_llm.generate(test_question)
        cloud_time = time.time() - start_time
        
        cloud_token_usage = cloud_llm.get_token_usage()
        print(f"   â±ï¸ å“åº”æ—¶é—´: {cloud_time:.2f}ç§’")
        print(f"   ğŸ”¢ ä½¿ç”¨Tokenæ•°: {cloud_result.tokens_used}")
        print(f"   ğŸ“Š æ€»Tokenç»Ÿè®¡: {cloud_token_usage['total_tokens_used']}")
        
        # æ€§èƒ½å¯¹æ¯”
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        print(f"   æœ¬åœ°æ¨¡å¼: {local_time:.2f}ç§’, {local_result.tokens_used} tokens")
        print(f"   äº‘ç«¯æ¨¡å¼: {cloud_time:.2f}ç§’, {cloud_result.tokens_used} tokens")
        print(f"   é€Ÿåº¦å·®å¼‚: {cloud_time/local_time:.2f}x")
        print(f"   Tokenæ•ˆç‡: {local_result.tokens_used/cloud_result.tokens_used:.2f}x")
    
    async def demo_agent(self):
        """Agentæ ·ä¾‹"""
        print("ğŸ¤– Agentæ ·ä¾‹")
        
        # åˆ›å»ºLLMå·¥å‚
        factory_config = LLMFactoryConfig(
            preferred_mode="auto",
            local_model_name="qwen3:8b",
            cloud_model_name="qwen-plus",
            api_key=self.api_key
        )
        factory = LLMFactory(factory_config)
        llm = await factory.create_llm()
        
        # é‡ç½®Tokenç»Ÿè®¡
        llm.reset_token_usage()
        
        # åˆ›å»ºè¾“å…¥çº¦æŸ
        input_constraints = [
            InputTypeConstraint(
                name="topic",
                data_type="string",
                min_length=1,
                max_length=200,
                description="è¦åˆ†æçš„ä¸»é¢˜"
            )
        ]
        
        # åˆ›å»ºè¾“å‡ºçº¦æŸ
        output_constraints = [
            OutputTypeConstraint(
                name="analysis",
                data_type="string",
                description="åˆ†æç»“æœ"
            )
        ]
        
        # åˆ›å»ºè¾“å‡ºæ ‡å‡†
        output_criterion = OutputCriterion(
            name="è´¨é‡è¯„ä¼°",
            description="è¾“å‡ºåº”è¯¥å‡†ç¡®ã€å®Œæ•´ä¸”æœ‰ç”¨",
            min_score=0.7
        )
        
        # åˆ›å»ºAgenté…ç½®
        agent_config = AgentConfig(
            name="æ™ºèƒ½åˆ†æAgent",
            description="åˆ†æç»™å®šä¸»é¢˜å¹¶æä¾›ä¸“ä¸šè§è§£",
            input_constraints=input_constraints,
            output_constraints=output_constraints,
            output_criterion=output_criterion
        )
        
        # åˆ›å»ºAgent
        agent = AgentBase(agent_config, llm)
        
        # æµ‹è¯•ä¸»é¢˜
        topics = [
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
            #"æœºå™¨å­¦ä¹ åœ¨é‡‘èè¡Œä¸šçš„åº”ç”¨",
            #"æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­çš„ä½œç”¨"
        ]
        
        for i, topic in enumerate(topics, 1):
            print(f"\nğŸ“ ä¸»é¢˜ {i}: {topic}")
            
            try:
                result = await agent.execute(topic)
                print(f"   ğŸ“„ åˆ†æç»“æœ: {result[:150]}...")
                
                # è·å–AgentçŠ¶æ€
                status = agent.get_status()
                print(f"   ğŸ“ˆ æ‰§è¡Œæ¬¡æ•°: {status['execution_count']}")
                print(f"   ğŸ“Š å¹³å‡åˆ†æ•°: {status['performance_metrics']['average_score']:.2f}")
                print(f"   ğŸ”¢ æ€»Tokenæ•°: {status['performance_metrics']['total_tokens_used']}")
                
            except Exception as e:
                print(f"   âŒ æ‰§è¡Œå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        final_status = agent.get_status()
        print(f"\nğŸ“Š Agentæœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»æ‰§è¡Œæ¬¡æ•°: {final_status['performance_metrics']['total_executions']}")
        print(f"   æˆåŠŸæ‰§è¡Œæ¬¡æ•°: {final_status['performance_metrics']['successful_executions']}")
        print(f"   å¹³å‡åˆ†æ•°: {final_status['performance_metrics']['average_score']:.2f}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {final_status['performance_metrics']['average_response_time']:.2f}ç§’")
        print(f"   æ€»Tokenä½¿ç”¨: {final_status['performance_metrics'].get('total_tokens_used', 0)}")
        print(f"   å¹³å‡Token/æ‰§è¡Œ: {final_status['performance_metrics'].get('average_tokens_per_execution', 0):.1f}")
    
    async def demo_token_monitoring(self):
        """Tokenç›‘æ§æ ·ä¾‹"""
        print("ğŸ” Tokenç›‘æ§æ ·ä¾‹")
        
        # åˆ›å»ºLLM
        config = LLMFactoryConfig(
            preferred_mode="cloud",
            cloud_model_name="qwen-plus",
            api_key=self.api_key
        )
        factory = LLMFactory(config)
        llm = await factory.create_llm()
        
        # é‡ç½®Tokenç»Ÿè®¡
        llm.reset_token_usage()
        
        print("ğŸ“Š åˆå§‹Tokenç»Ÿè®¡:")
        initial_usage = llm.get_token_usage()
        print(f"   æ€»Tokenæ•°: {initial_usage['total_tokens_used']}")
        print(f"   æ€»è¯·æ±‚æ•°: {initial_usage['total_requests']}")
        
        # æ‰§è¡Œå¤šä¸ªè¯·æ±‚
        requests = [
            "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ ",
            "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ"
        ]
        
        print(f"\nğŸš€ æ‰§è¡Œ {len(requests)} ä¸ªè¯·æ±‚...")
        
        for i, request in enumerate(requests, 1):
            print(f"\nğŸ“ è¯·æ±‚ {i}: {request}")
            
            result = await llm.generate(request)
            
            # å®æ—¶æ˜¾ç¤ºTokenä½¿ç”¨æƒ…å†µ
            current_usage = llm.get_token_usage()
            print(f"   ğŸ”¢ æœ¬æ¬¡Token: {result.tokens_used}")
            print(f"   ğŸ“Š ç´¯è®¡Token: {current_usage['total_tokens_used']}")
            print(f"   ğŸ“ˆ å¹³å‡Token/è¯·æ±‚: {current_usage['average_tokens_per_request']:.1f}")
        
        # æœ€ç»ˆç»Ÿè®¡
        final_usage = llm.get_token_usage()
        print(f"\nğŸ“Š æœ€ç»ˆTokenç»Ÿè®¡:")
        print(f"   æ€»Tokenæ•°: {final_usage['total_tokens_used']}")
        print(f"   æ€»è¯·æ±‚æ•°: {final_usage['total_requests']}")
        print(f"   å¹³å‡Token/è¯·æ±‚: {final_usage['average_tokens_per_request']:.1f}")
        
        # ä¼°ç®—æˆæœ¬ï¼ˆå‡è®¾æ¯1000 tokens $0.01ï¼‰
        estimated_cost = final_usage['total_tokens_used'] * 0.01 / 1000
        print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${estimated_cost:.4f}")
    
    async def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰æ ·ä¾‹"""
        print("ğŸ¯ è¿è¡Œæ‰€æœ‰æ ·ä¾‹")
        
        demos = [
            ("è¿é€šæ€§æµ‹è¯•", self.test_connectivity),
            ("æœ¬åœ°æ¨¡å‹æ ·ä¾‹", self.demo_local_model),
            ("äº‘ç«¯æ¨¡å‹æ ·ä¾‹", self.demo_cloud_model),
            ("æ€§èƒ½å¯¹æ¯”æ ·ä¾‹", self.demo_performance_comparison),
            ("Agentæ ·ä¾‹", self.demo_agent),
            ("Tokenç›‘æ§æ ·ä¾‹", self.demo_token_monitoring)
        ]
        
        for name, func in demos:
            await self.run_demo(name, func)
            print()  # æ·»åŠ ç©ºè¡Œåˆ†éš”
    
    async def run(self):
        """è¿è¡Œæ¼”ç¤ºæ¡†æ¶"""
        while True:
            self.show_menu()
            
            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-7): ").strip()
                
                if choice == "0":
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if choice in self.demo_modules:
                    name, func = self.demo_modules[choice]
                    await self.run_demo(name, func)
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    demo = DemoFramework()
    await demo.run()


if __name__ == "__main__":
    asyncio.run(main()) 